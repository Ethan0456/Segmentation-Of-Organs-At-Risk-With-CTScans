{"metadata":{"accelerator":"GPU","colab":{"provenance":[]},"kernelspec":{"name":"python3","display_name":"Python 3","language":"python"},"language_info":{"name":"python","version":"3.10.12","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"nvidiaTeslaT4","dataSources":[{"sourceId":6843024,"sourceType":"datasetVersion","datasetId":3933872}],"dockerImageVersionId":30588,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":true}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"!pip3 install nibabel\n!pip3 install segmentation-models-3d\n!pip install -U segmentation-models-pytorch\n!pip install -U torchmetrics","metadata":{"id":"GHbdvEqaj-uD","outputId":"cb7305ac-364f-4b33-f6c5-f5d35d98db17","execution":{"iopub.status.busy":"2023-11-17T06:52:36.071207Z","iopub.execute_input":"2023-11-17T06:52:36.071486Z","iopub.status.idle":"2023-11-17T06:54:25.061589Z","shell.execute_reply.started":"2023-11-17T06:52:36.071459Z","shell.execute_reply":"2023-11-17T06:54:25.060454Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Import Libraries and Load Data","metadata":{"id":"Vu2scHtwsrYc"}},{"cell_type":"code","source":"import numpy as np\nimport torch\nimport nibabel as nib\nfrom torch.utils.data import Dataset, DataLoader\n\ndevice = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\ndevice","metadata":{"id":"kI7zkR38j4h5","execution":{"iopub.status.busy":"2023-11-17T06:54:25.064158Z","iopub.execute_input":"2023-11-17T06:54:25.064995Z","iopub.status.idle":"2023-11-17T06:54:28.874566Z","shell.execute_reply.started":"2023-11-17T06:54:25.064959Z","shell.execute_reply":"2023-11-17T06:54:28.873518Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"import glob\n\ndata_path = '/kaggle/input/'\n\nvolumes = [file for file in glob.glob(data_path+'volume-*')]\nlabels = [file for file in glob.glob(data_path+'labels-*')]\n\n# volumes.extend(glob.glob(data_path+'volume-[01].nii'))\n# labels.extend(glob.glob(data_path+'labels-[01].nii'))\n\nvolumes = sorted(volumes)\nlabels = sorted(labels)\nvolumes, labels","metadata":{"id":"ym93ClRckDa8","outputId":"3e62c121-be24-4354-b77c-c58fb8c3853b","execution":{"iopub.status.busy":"2023-11-17T06:54:28.875688Z","iopub.execute_input":"2023-11-17T06:54:28.876110Z","iopub.status.idle":"2023-11-17T06:54:28.889814Z","shell.execute_reply.started":"2023-11-17T06:54:28.876082Z","shell.execute_reply":"2023-11-17T06:54:28.889003Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Data Information","metadata":{}},{"cell_type":"code","source":"# data = torch.Tensor(nib.load('/content/drive/MyDrive/Datasets/volume-0.nii.gz').get_fdata()).to(device)\n# print(data.shape)\n# patches = pt.patchify(data, (8,8,8), step=8)\n# patches.shape\n# patches = data.unfold(2, 8, 8).unfold(1, 8, 8).unfold(0, 8, 8)\n# patches = patches.contiguous().view(-1, 8, 8, 8)\n# print(patches.shape)\n# patches = patches.view(-1,1,8,8,8)\n# print(patches.shape)","metadata":{"id":"7fBJiab-nxAO","execution":{"iopub.status.busy":"2023-11-17T06:54:28.892492Z","iopub.execute_input":"2023-11-17T06:54:28.892857Z","iopub.status.idle":"2023-11-17T06:54:28.897505Z","shell.execute_reply.started":"2023-11-17T06:54:28.892824Z","shell.execute_reply":"2023-11-17T06:54:28.896474Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Custom Dataset Class","metadata":{"id":"biU9zgcDsohV"}},{"cell_type":"code","source":"import os\nimport shutil\n\nclass CustomDataset(Dataset):\n    def __init__(self, volumes, labels, transform=None):\n        self.volumes = volumes\n        self.labels = labels\n\n    def __getitem__(self, idx):\n        return self.volumes[idx], self.labels[idx]\n\n    def __len__(self):\n        return len(self.volumes)\n\nclass CustomPatchDataset(Dataset):\n    def __init__(self, volume_patches, label_patches, transform=None):\n        self.vol_patches = volume_patches\n        self.label_patches = label_patches\n\n    def __getitem__(self, idx):\n        return self.vol_patches[idx], self.label_patches[idx]\n\n    def __len__(self):\n        return len(self.vol_patches)","metadata":{"id":"A3jth1PGkjDn","execution":{"iopub.status.busy":"2023-11-17T06:54:28.898850Z","iopub.execute_input":"2023-11-17T06:54:28.899337Z","iopub.status.idle":"2023-11-17T06:54:28.908873Z","shell.execute_reply.started":"2023-11-17T06:54:28.899303Z","shell.execute_reply":"2023-11-17T06:54:28.908123Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"dataset = CustomDataset(volumes, labels)\ndataloader = DataLoader(dataset, batch_size=1, shuffle=True)","metadata":{"id":"SLpvRI-clOTH","execution":{"iopub.status.busy":"2023-11-17T06:54:28.909958Z","iopub.execute_input":"2023-11-17T06:54:28.910245Z","iopub.status.idle":"2023-11-17T06:54:28.920740Z","shell.execute_reply.started":"2023-11-17T06:54:28.910213Z","shell.execute_reply":"2023-11-17T06:54:28.920006Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Model Implementation","metadata":{"id":"3rNSYwdAsmz_"}},{"cell_type":"code","source":"import torch.nn as nn\n\nclass conv_block(nn.Module):\n    def __init__(self, in_channels, out_channels):\n        super().__init__()\n\n        self.conv1 = nn.Conv3d(in_channels, out_channels, kernel_size=3, padding=1)\n        self.bn1 = nn.BatchNorm3d(out_channels)\n        self.relu = nn.ReLU()\n\n    def forward(self, inputs):\n        x = self.conv1(inputs)\n        x = self.bn1(x)\n        x = self.relu(x)\n\n        return x\n\nclass encoder_block(nn.Module):\n    def __init__(self, in_channels, out_channels):\n        super().__init__()\n\n        self.conv = conv_block(in_channels, out_channels)\n        self.pool = nn.MaxPool3d(kernel_size=2)\n\n    def forward(self, inputs):\n        x = self.conv(inputs)\n        p = self.pool(x)\n\n        return x, p\n\nclass decoder_block(nn.Module):\n    def __init__(self, in_channels, out_channels):\n        super().__init__()\n        self.up = nn.ConvTranspose3d(in_channels, out_channels, kernel_size=2, stride=2, padding=0)\n        self.conv = conv_block(out_channels+out_channels, out_channels)\n\n    def forward(self, inputs, skip):\n        x = self.up(inputs)\n        x = torch.cat([x, skip], axis=1)\n        x = self.conv(x)\n\n        return x\n\nclass build_unet(nn.Module):\n    def __init__(self):\n        super().__init__()\n        \"\"\" Encoder \"\"\"\n        self.e1 = encoder_block(6,16)\n        self.e2 = encoder_block(16,32)\n        self.e3 = encoder_block(32,64)\n        self.e4 = encoder_block(64,256)\n        # self.e5 = encoder_block(128,256)\n\n        \"\"\" Bottleneck \"\"\"\n        self.b = encoder_block(256,512)\n\n        \"\"\" Decoder \"\"\"\n        self.d0 = decoder_block(512, 256)\n        self.d1 = decoder_block(256, 64)\n        # self.d2 = decoder_block(128, 64)\n        self.d3 = decoder_block(64, 32)\n        self.d4 = decoder_block(32, 16)\n\n        # More Complex Model\n        \"\"\" Encoder \"\"\"\n        # self.e1 = encoder_block(6,16)\n        # self.e2 = encoder_block(16,32)\n        # self.e3 = encoder_block(32,64)\n        # self.e4 = encoder_block(64,128)\n        # self.e5 = encoder_block(128,256)\n\n        # \"\"\" Bottleneck \"\"\"\n        # self.b = encoder_block(256,512)\n\n        # \"\"\" Decoder \"\"\"\n        # self.d0 = decoder_block(512, 256)\n        # self.d1 = decoder_block(256, 128)\n        # self.d2 = decoder_block(128, 64)\n        # self.d3 = decoder_block(64, 32)\n        # self.d4 = decoder_block(32, 16)\n\n        \"\"\" Classifier \"\"\"\n        self.outputs = nn.Conv3d(16, 6, kernel_size=1, padding=0)\n\n\n    def forward(self, inputs):\n        \"\"\" Encoder \"\"\"\n        s1, p1 = self.e1(inputs)\n        s2, p2 = self.e2(p1)\n        s3, p3 = self.e3(p2)\n        s4, p4 = self.e4(p3)\n        # s5, p5 = self.e5(p4)\n\n        \"\"\" Bottleneck \"\"\"\n        b, b1 = self.b(p4)\n\n        \"\"\" Decoder \"\"\"\n        d0 = self.d0(b, s4)\n        d1 = self.d1(d0, s3)\n        # d2 = self.d2(d1, s3)\n        d3 = self.d3(d1, s2)\n        d4 = self.d4(d3, s1)\n\n        \"\"\" Classifier \"\"\"\n        outputs = self.outputs(d4)\n\n        return outputs\n","metadata":{"id":"HZuMnrw5fKe3","execution":{"iopub.status.busy":"2023-11-17T06:54:28.922123Z","iopub.execute_input":"2023-11-17T06:54:28.922809Z","iopub.status.idle":"2023-11-17T06:54:28.940238Z","shell.execute_reply.started":"2023-11-17T06:54:28.922778Z","shell.execute_reply":"2023-11-17T06:54:28.939417Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Training Model on 2 GPU (30GB) with PyTorch DataParallel","metadata":{}},{"cell_type":"code","source":"import torch\nimport torch.nn as nn\nimport torch.optim as optim\nimport torch.nn.functional as F\nfrom torchvision import datasets, transforms\nfrom torch.utils.data import DataLoader\nfrom tqdm import tqdm\nfrom segmentation_models_pytorch.losses import FocalLoss as fl, DiceLoss as dl\nfrom segmentation_models_pytorch.metrics.functional import iou_score\nfrom torchmetrics import JaccardIndex, Dice\nimport matplotlib.pyplot as plt\nfrom matplotlib.colors import ListedColormap\n\ncolors = ['black', 'green', 'red', 'blue', 'purple', 'orange', 'yellow']\ncustom_cmap = ListedColormap(colors)\n\nmodel = nn.DataParallel(build_unet()).to(device)\noptimizer = optim.Adam(model.parameters(), lr=0.001)\n\nloss_obj = fl(mode='multiclass', gamma=2).to(device)\nloss_dice = Dice(multiclass=True, num_classes=6).to(device)\nmetric_iou = JaccardIndex(num_classes=6, task=\"multiclass\").to(device)\n\n# Training loop\nnum_epochs = 100\nfor epoch in range(num_epochs):\n    model.train()\n    total_iou = 0.0\n    total_loss = 0.0\n    total_patches_epoch = 0\n    print('#'*50)\n    print('-'*20, f'EPOCH {epoch}', '-'*20)\n\n    for batch_idx, (data_path, target_path) in enumerate(dataloader):\n        print('='*50)\n        print(f'Working on : {data_path} {target_path}')\n        print('-'*50)\n        volume = torch.Tensor(nib.load(data_path[0]).get_fdata()).to(device)\n        label = torch.Tensor(nib.load(target_path[0]).get_fdata()).to(device)\n\n        # Patchify\n        volume_patches = volume.unfold(2,64,64).unfold(1,64,64).unfold(0,64,64)\n        label_patches = label.unfold(2,64,64).unfold(1,64,64).unfold(0,64,64)\n\n        volume_patches = volume_patches.contiguous().view(-1,1,64,64,64)\n        label_patches = label_patches.contiguous().view(-1,1,64,64,64)\n\n        patch_dataset = CustomPatchDataset(volume_patches, label_patches)\n        patch_dataloader = DataLoader(patch_dataset, batch_size=32)\n\n        total_patches = len(patch_dataset)\n        patch_loader_size = len(patch_dataloader)\n        total_patches_epoch += total_patches\n        print('Total Patches', total_patches, 'Dataset Loader', patch_loader_size)\n\n        for patch_idx, (data, target) in zip(tqdm(np.arange(patch_loader_size), total=patch_loader_size), patch_dataloader):\n            batch_loss = 0.0\n            batch_iou = 0.0\n\n            # for data, target in patch_dataloader:\n            data = data.expand(-1,6,-1,-1,-1)\n            # target = favourable_target(target)\n\n            optimizer.zero_grad()\n            output = model(data)\n\n            # print(output.shape, target.shape)\n            loss = loss_obj(output, target.squeeze()) + loss_dice(output, target.to(torch.int).squeeze())\n\n            loss.backward()\n            optimizer.step()\n\n            # Calculate IOU for the current batch\n            pred = torch.argmax(output, dim=1)\n\n            iou_patch = []\n            iou_patch.append(metric_iou(output, target.squeeze()))\n\n            actual_iou_patch = sum(iou_patch)/len(iou_patch)\n\n            total_iou += actual_iou_patch\n            total_loss += loss.item()\n\n            batch_iou += actual_iou_patch\n            batch_loss += loss.item()\n\n            # Extract the center slices (32, 32) from both images\n            output_cp = output.detach().clone()\n\n            target_cpu = target.cpu().numpy()\n            output_cpu = output_cp.cpu().numpy()\n\n            output_cpu = np.argmax(output_cpu, axis=1)\n\n            # Now, single_channel_image contains integer values from 0 to 5, corresponding to different organs.\n            center_slice_actual = target_cpu.squeeze()[16, 32, :, :]\n            center_slice_predicted = output_cpu[16, 32, :, :]\n\n            # Create a figure and plot the actual and predicted single-channel images side by side\n            plt.figure(figsize=(12, 5))\n\n            plt.subplot(1, 2, 1)\n            plt.imshow(center_slice_actual, cmap=custom_cmap, vmin=0, vmax=5)\n            plt.title('Actual Center Slice')\n\n            plt.subplot(1, 2, 2)\n            plt.imshow(center_slice_predicted, cmap=custom_cmap, vmin=0, vmax=5)\n            plt.title('Predicted Center Slice')\n\n            plt.savefig(f'segment-p{patch_idx}-b{batch_idx}-e{epoch}.png')\n            plt.close()\n\n            avg_batch_loss = batch_loss / len(patch_dataloader)\n            avg_batch_metric = batch_iou / len(patch_dataloader)\n            torch.save(model.state_dict(), 'unet3d_model.pth')\n\n            print(f\"Epoch [{epoch+1}/{num_epochs}] Batch [{batch_idx+1}/{len(dataloader)}] Loss: {avg_batch_loss:.4f} IOU: {avg_batch_metric:.4f}\")\n            print()\n\n    # Calculate average metrics for the epoch\n    avg_iou = total_iou / total_patches_epoch\n    avg_loss = total_loss / total_patches_epoch\n\n    print(f\"Epoch [{epoch+1}/{num_epochs}] Avg Loss: {avg_loss:.4f} Avg IOU: {avg_iou:.4f}\")\n\n# Save the trained model if needed\ntorch.save(model.state_dict(), 'unet3d_model.pth')","metadata":{"id":"WfbV-3f9tHCO","outputId":"0321a122-7973-477b-b7e7-0050e2fe0323","execution":{"iopub.status.busy":"2023-11-17T06:54:28.941537Z","iopub.execute_input":"2023-11-17T06:54:28.942147Z","iopub.status.idle":"2023-11-17T08:14:33.020510Z","shell.execute_reply.started":"2023-11-17T06:54:28.942114Z","shell.execute_reply":"2023-11-17T08:14:33.019121Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# del model\n# del optimizer\n# torch.cuda.empty_cache()\n# import gc\n# gc.collect()","metadata":{"id":"VTai2K6QtXFM","execution":{"iopub.status.busy":"2023-11-17T08:14:33.021860Z","iopub.status.idle":"2023-11-17T08:14:33.022323Z","shell.execute_reply.started":"2023-11-17T08:14:33.022033Z","shell.execute_reply":"2023-11-17T08:14:33.022049Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"!zip data-color.zip /kaggle/working/*","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"import os\nimport subprocess\nfrom IPython.display import FileLink, display\n\ndef download_file(path, download_file_name):\n    os.chdir('/kaggle/working/')\n    zip_name = f\"/kaggle/working/{download_file_name}.zip\"\n#     command = f\"zip {zip_name} {path} -r\"\n#     result = subprocess.run(command, shell=True, capture_output=True, text=True)\n#     if result.returncode != 0:\n#         print(\"Unable to run zip command!\")\n#         print(result.stderr)\n#         return\n    display(FileLink(f'{download_file_name}.zip'))\ndownload_file('', 'data-color')","metadata":{"execution":{"iopub.status.busy":"2023-11-17T08:16:42.324799Z","iopub.execute_input":"2023-11-17T08:16:42.325840Z","iopub.status.idle":"2023-11-17T08:16:42.334283Z","shell.execute_reply.started":"2023-11-17T08:16:42.325800Z","shell.execute_reply":"2023-11-17T08:16:42.333424Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]}]}